# Load in a raw body of text and set it up for training
# In this problem we will use Amazon product reviews and Tweets from X for training

# Your task is to encode the input dataset of strings as an integer tensor of size 2⋅N×T, where T is the length of the longest string. 
# The lexicographically first word should be represented as 1, the second should be 2, 
# and so on. In the final tensor, list the positive encodings, in order, before the negative encodings.

import torch
import torch.nn as nn
from torchtyping import TensorType

class Solution:
    def get_dataset(self, positive: List[str], negative: List[str]) -> TensorType[float]:
        # Total set of words
        words = set()
        combined = positive + negative
        for sentence in combined:
            for word in sentence.split():
                words.add(word)

        # Build a mapping
        sorted_list = sorted(list(words))
        word_to_int = {}
        for i, c in enumerate(sorted_list):
            word_to_int[c] = i + 1

        # Write encode() which is used to build the dataset
        def encode(sentence):
            integers = []
            for word in sentence.split():
                integers.append(word_to_int[word])
            return integers
        
        var_len_tensors = []
        for sentence in combined:
            var_len_tensors.append(torch.tensor(encode(sentence)))
        
        return nn.utils.rnn.pad_sequence(var_len_tensors, batch_first = True)