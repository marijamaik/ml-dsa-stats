import torch
import torch.nn as nn
from torchtyping import TensorType

# Transformer block with attention and feedforward sublayers
class TransformerBlock(nn.Module):
    def __init__(self, model_dim: int, num_heads: int):
        super().__init__()
        torch.manual_seed(0)  # ensure reproducibility
        self.attention = self.MultiHeadedSelfAttention(model_dim, num_heads)
        self.linear_network = self.VanillaNeuralNetwork(model_dim)
        self.first_norm = nn.LayerNorm(model_dim)
        self.second_norm = nn.LayerNorm(model_dim)

    def forward(self, embedded: TensorType[float]) -> TensorType[float]:
        torch.manual_seed(0)  # ensure consistent output
        # Apply attention with residual connection
        embedded = embedded + self.attention(self.first_norm(embedded))
        # Apply feedforward network with another residual
        embedded = embedded + self.linear_network(self.second_norm(embedded))
        # Round for clarity/consistency (useful in toy models or visualization)
        return torch.round(embedded, decimals=4)

    # Multi-head self-attention composed of several single-head attentions
    class MultiHeadedSelfAttention(nn.Module):
        
        # A single attention head: computes scaled dot-product attention
        class SingleHeadAttention(nn.Module):
            def __init__(self, model_dim: int, head_size: int):
                super().__init__()
                torch.manual_seed(0)
                # Linear projections to generate keys, queries, values
                self.key_gen = nn.Linear(model_dim, head_size, bias=False)
                self.query_gen = nn.Linear(model_dim, head_size, bias=False)
                self.value_gen = nn.Linear(model_dim, head_size, bias=False)
            
            def forward(self, embedded: TensorType[float]) -> TensorType[float]:
                k = self.key_gen(embedded)
                q = self.query_gen(embedded)
                v = self.value_gen(embedded)

                # Compute scaled dot-product attention scores
                scores = q @ torch.transpose(k, 1, 2)
                scores /= k.shape[2] ** 0.5  # scale by sqrt(head size)

                # Apply causal mask to prevent attending to future tokens
                mask = torch.tril(torch.ones(k.shape[1], k.shape[1])) == 0
                scores = scores.masked_fill(mask, float('-inf'))

                # Convert scores to probabilities
                scores = nn.functional.softmax(scores, dim=2)

                # Weighted sum of values
                return scores @ v

        def __init__(self, model_dim: int, num_heads: int):
            super().__init__()
            torch.manual_seed(0)
            # Create multiple attention heads
            self.att_heads = nn.ModuleList([
                self.SingleHeadAttention(model_dim, model_dim // num_heads)
                for _ in range(num_heads)
            ])

        def forward(self, embedded: TensorType[float]) -> TensorType[float]:
            # Run input through each head and concatenate outputs
            head_outputs = [head(embedded) for head in self.att_heads]
            return torch.cat(head_outputs, dim=2)

    # Simple 2-layer feedforward network with dropout
    class VanillaNeuralNetwork(nn.Module):
        def __init__(self, model_dim: int):
            super().__init__()
            torch.manual_seed(0)
            self.up_projection = nn.Linear(model_dim, model_dim * 4)
            self.relu = nn.ReLU()
            self.down_projection = nn.Linear(model_dim * 4, model_dim)
            self.dropout = nn.Dropout(0.2)

        def forward(self, x: TensorType[float]) -> TensorType[float]:
            torch.manual_seed(0)
            # Project up, apply ReLU, project back down, and apply dropout
            return self.dropout(self.down_projection(self.relu(self.up_projection(x))))
