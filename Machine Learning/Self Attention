import torch
import torch.nn as nn
from torchtyping import TensorType

# Implement single-head causal self-attention using scaled dot-product attention with masking
class SingleHeadAttention(nn.Module):
    
    def __init__(self, embedding_dim: int, attention_dim: int):
        super().__init__()
        torch.manual_seed(0)
        # Linear layers to generate keys, queries, and values
        self.key_gen = nn.Linear(embedding_dim, attention_dim, bias=False)
        self.query_gen = nn.Linear(embedding_dim, attention_dim, bias=False)
        self.value_gen = nn.Linear(embedding_dim, attention_dim, bias=False)
    
    def forward(self, embedded: TensorType[float]) -> TensorType[float]:
        # Compute keys, queries, and values
        k = self.key_gen(embedded)
        q = self.query_gen(embedded)
        v = self.value_gen(embedded)

        # Scaled dot-product attention scores
        scores = q @ torch.transpose(k, 1, 2) # @ is the same as torch.matmul()
        context_length, attention_dim = k.shape[1], k.shape[2]
        scores = scores / (attention_dim ** 0.5)

        # Causal mask to prevent attending to future tokens
        lower_triangular = torch.tril(torch.ones(context_length, context_length))
        mask = lower_triangular == 0
        scores = scores.masked_fill(mask, float('-inf'))
        # Apply softmax to get attention weights
        scores = nn.functional.softmax(scores, dim = 2)

        # Compute the weighted sum of values
        return torch.round(scores @ v, decimals=4)
