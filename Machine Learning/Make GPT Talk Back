import torch
import torch.nn as nn
from torchtyping import TensorType

# Generates a deterministic sequence of characters from a model by autoregressively sampling tokens based on predicted probabilities
class Solution:
    def generate(self, model, new_chars: int, context: TensorType[int], context_length: int, int_to_char: dict) -> str:
        # Initialise a manual random number generator with a fixed seed for reproducibility
        generator = torch.manual_seed(0)
        # Save the initial RNG state to reuse it for consistent sampling
        initial_state = generator.get_state()
        res = []
        for i in range(new_chars):
            # Ensure context length doesn't exceed the model's maximum allowed input length
            if len(context.T) > context_length:
                # Truncate to the most recent tokens
                context = context[:, -context_length:]

            # Forward pass through the model to get predictions for each position (B, T, Vocab_Size)
            prediction = model(context)
            # Extract predictions for the last token in the sequence (B, Vocab_Size)
            last_time_step = prediction[:, -1, :]
            # Convert logits to probability distribution over vocabulary
            probabilities = nn.functional.softmax(last_time_step, dim = -1)
            # Sample the next token based on predicted probabilities
            next_char = torch.multinomial(probabilities, 1, generator=generator)
            # Reset RNG state to ensure deterministic output across iterations
            generator.set_state(initial_state)
            # Append the sampled token to the context for the next iteration
            context = torch.cat((context, next_char), dim = -1)
             # Convert token index to character and add to result list
            res.append(int_to_char[next_char.item()])
        # Join all characters to form the final generated string
        return ''.join(res)
